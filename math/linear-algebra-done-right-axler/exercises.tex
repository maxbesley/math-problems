\documentclass{article}

\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{extramarks}
\usepackage[english]{babel}

%
% Basic Document Settings
%
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\rhead{\textbf{\authorName}}
\chead{\textbf{Exercises for \textit{\bookTitle}}}
\cfoot{\thepage}
\lhead{\firstxmark}
\lfoot{\lastxmark}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Exercise Sections
%
\newcommand{\enterExerciseHeader}[1]{
    \nobreak\extramarks{}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitExerciseHeader}[1]{
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Exercise \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{exerciseCounter}
\setcounter{exerciseCounter}{1}
\nobreak\extramarks{Exercise \arabic{exerciseCounter}}{}\nobreak{}

\newenvironment{exercise}[1][-1]{
    \ifnum#1>0
        \setcounter{exerciseCounter}{#1}
    \fi
    \section{Exercise \arabic{exerciseCounter}}
    \setcounter{partCounter}{1}
    \enterExerciseHeader{exerciseCounter}
}{
    \exitExerciseHeader{exerciseCounter}
}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\bookTitle}{Linear Algebra Done Right}
\newcommand{\bookAuthor}{Sheldon Axler}
\newcommand{\authorName}{Max Besley}

%
% Title Page
%
\title{
    \vspace{2in}
    \textmd{\textbf{\bookTitle:\ Exercises}}\\
    \vspace{0.1in}\large{\textbf{Textbook by \bookAuthor}}
    \vspace{3in}
}

\author{\textbf{\authorName}}
\date{}

\renewcommand\qedsymbol{$\blacksquare$}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

\begin{document}

\maketitle

\pagebreak

\section{\underline{\Large{Exercises 1A}}} \vspace{5mm}

\begin{exercise}[6]
    \begin{proof}
        Assume $\alpha = a+bi \in \C$ and $\alpha \neq 0$.

        Let $\beta = \dfrac{a-bi}{a^2+b^2}$. Then using the arithmetic
        properties of complex numbers we can easily verify $\alpha\beta = 1$.

        If $\alpha\gamma = 1$ and $\alpha\beta = 1$ then $\alpha\gamma
        = \alpha\beta$ and multiplying both sides by $\alpha^{-1} \in \C$
        on the left gives $\alpha = \gamma$, showing that this multiplicative inverse
        is unique.
    \end{proof}
\end{exercise}

\begin{exercise}[7]
    We compute,

    \begin{equation*}
    \begin{split}
        \left( \dfrac{-1+\sqrt{3}i}{2} \right)^3
        & = \dfrac{1}{8} \left( -2 - 2\sqrt{3}i \right) \left( -1 + \sqrt{3}i \right) \\
        & = \dfrac{1}{8} \left( 2 + -2\sqrt{3}i + 2\sqrt{3}i + 6 \right) \\
        & = 1
    \end{split}
    \end{equation*}
\end{exercise}

\begin{exercise}[8]
    Two square roots of the imaginary unit $i$ are $z_1 = \frac{\sqrt{2}}{2} +\frac{\sqrt{2}}{2}i$ and $z_2 = -z_1$.

    It is easy to check that indeed $z_1^2 = i = (-z_1)^2$.
\end{exercise}

\pagebreak

\section{\underline{\Large{Exercises 1B}}} \vspace{5mm}

\begin{exercise}[2]
    \begin{proof}
        Suppose $a \in \F$, $\vec{v} \in V$, and $a\vec{v} = \vec{0}$.
        Assume for the sake of contradiction that $a \neq 0$ and $\vec{v} \neq \vec{0}$.
        By \textsc{Theorem 1.31} we have $a\vec{0} = \vec{0}$.
        Also because $a \neq 0$ we know $a^{-1} \in \F$ exists.
        Multiplying each side of the equation $a\vec{v} = a\vec{0}$ by $a^{-1}$
        on the left gives $\vec{v} = \vec{0}$, a contradiction.
        Thus, $a = 0$ or $\vec{v} = \vec{0}$.
    \end{proof}
\end{exercise}

\begin{exercise}[4]
    The empty set fails to satisfy the \textit{additive identity}
    property and thus is not a vector space. Every other vector
    space property is vacuously true in this case.
\end{exercise}

\begin{exercise}[5]
    We prove that
    \begin{align*}
        \text{$0\vec{v}=\vec{0}$ for all $\vec{v} \in V$} \iff
        \text{For every $\vec{v} \in V$ there exists $\vec{w}
        \in V$ such that $\vec{v}+\vec{w}=\vec{0}$} \\
    \end{align*}
    \begin{proof}
        ($\implies$):
        We compute
        \begin{equation*}
            \vec{0}=0\vec{v}=(1+(-1))\vec{v}=1\vec{v}+(-1)\vec{v}=\vec{v}+(-1)\vec{v}
        \end{equation*}
        By the closure of scalar multiplication $(-1)\vec{v} \in V$, proving this direction. \\\\
        ($\impliedby$):
        We have
        \begin{equation*}
            \vec{0}+\vec{v}=\vec{v}=1\vec{v}=(0+1)\vec{v}=0\vec{v}+1\vec{v}=0\vec{v}+\vec{v}
        \end{equation*}
        Now adding the additive inverse $-\vec{v} \in V$ to both sides gives $\vec{0}=0\vec{v}$.
        Completing the proof.
    \end{proof}
\end{exercise}

\begin{exercise}[8]
    Let $\vec{u_1},\vec{u_2},\vec{v_1},\vec{v_2} \in V$. \\
    Showing associativity and distributivity is too tedious and is omitted. \\\\
    \textbf{Commutativity:} \\
    \begin{equation*}
    \begin{split}
        (\vec{u_1}+i\vec{v_1})+(\vec{u_2}+i\vec{v_2}) & = (\vec{u_1}+\vec{u_2})+i(\vec{v_1}+\vec{v_2}) \\
                              & = (\vec{u_2}+\vec{u_1})+i(\vec{v_2}+\vec{v_1}) \\
                              & = (\vec{u_2}+i\vec{v_2})+(\vec{u_1}+i\vec{v_1})
    \end{split}
    \end{equation*} \\
    \textbf{Additive identity:} \\
    The vector $\vec{0}+i\vec{0}$ is the additive identity of $V_{\C}$. \\\\
    \textbf{Additive inverse:} \\
    Given $\vec{u}+i\vec{v} \in V_{\C}$ the additive inverse of
    this vector is $-\vec{u}+i(-\vec{v}) \in V_{\C}$. \\\\
    \textbf{Multiplicative identity:} \\
    \begin{equation*}
    \begin{split}
        (1)(\vec{u}+i\vec{v}) & = (1+i0)(\vec{u}+i\vec{v}) \\
                              & = (1\vec{u}+0\vec{v})+i(1\vec{v}+0\vec{u}) \\
                              & = \vec{u}+i\vec{v}
    \end{split}
    \end{equation*}
    and so $1 \in \C$ is the multiplicative identity of $V_{\C}$. \\\\
    Hence the complexification of $V$, denoted $V_{\C}$, is a vector space over $\C$.
\end{exercise}

\pagebreak

\section{\underline{\Large{Exercises 1C}}} \vspace{5mm}

\begin{exercise}[4]
    \begin{proof}
        ($\implies$):
        Assume this set of functions $W$ is a subspace of $\R^{[0,1]}$. \\
        By this assumption the zero function $0(x) = 0\ \forall x \in \R$
        is contained in W and $\int_0^1 0 = b$. But clearly $\int_0^1 0 = 0$
        and thus $b = 0$.

        ($\impliedby$):
        Assume $b = 0$. We have,
        \begin{enumerate}
            \item $\int_0^1 0 = 0$
            \item $\int_0^1 f+g = \int_0^1 f + \int_0^1 g = 0+0 = 0$
            \item $\int_0^1 \lambda f = \lambda \int_0^1 f = \lambda 0 = 0$
                  where $\lambda \in \F$
        \end{enumerate}
        So this subset of $\R^{[0,1]}$ is a subspace by \textsc{Theorem 1.34}.
    \end{proof}
\end{exercise}

\begin{exercise}[5]
    No. Here the field of scalars is $\C$ and so the subset
    $\R^2 \subseteq \C^2$ fails to be closed under scalar
    multiplication. \\
    On the other hand, $\R^2$ \textit{is} a subspace
    of the \textit{real} vector space $\C^2$.
\end{exercise}

\begin{exercise}[9]
    In general, the set of periodic functions from $\R$ to
     $\R$ does not form a subspace, because functions with
    different periods may form a sum that is not periodic. \\
    However, if we fix a real number $q \in \R$ then the
    set $\{f: \R \to \R \mid \textit{f is q-periodic}\}$
    forms a subspace of $\R^{\R}$.
\end{exercise}

\begin{exercise}[12]
    \begin{proof}
        ($\impliedby$):
        Suppose $U$ and $W$ are subspaces of $V$ and $U \subseteq W$.
        Then $U \cup W = W$ which is a subspace of $V$ by assumption. \\

        ($\implies$):
        Suppose $U \cup W$ is a subspace of V, where $U$ and $W$ are subspaces.
        For the sake of contradiction further suppose $U \nsubseteq W$ and $W \nsubseteq U$.
        Then there is a $\vec{u} \in U$ with $\vec{u} \notin W$ and a $\vec{w} \in W$ with $\vec{w} \notin U$.
        Now since $\vec{u} \in U \cup W$ and $\vec{w} \in U \cup W$ by
        closure under addition we have $\vec{u}+\vec{w} \in U \cup W$.
        However, if $\vec{u}+\vec{w} \in U$ then by closure $\vec{u}+\vec{w}-\vec{u} = \vec{w} \in U$, a contradiction.
        Likewise if $\vec{u}+\vec{w} \in W$. \\
        Thus we must have $U \subseteq W$ or $W \subseteq U$.
    \end{proof}
\end{exercise}

\begin{exercise}[15]
    If $U$ is a linear subspace then $U + U = U$.
    \begin{proof}
        By \textsc{Theorem 1.40} we have $U \subseteq U + U$.
        Now assume $\vec{x} + \vec{y} \in U + U$.
        Then by closure under vector addition $\vec{x} + \vec{y}
        \in U$ and so $U + U \subseteq U$.
    \end{proof}
\end{exercise}

\begin{exercise}[18]
    In this case the additive identity is the trivial subspace: $U + \{\vec{0}\} = U$. \\
    Intuitively, if $U + W = \{\vec{0}\}$ then every pair of elements of $U$ and $W$ are
    additive inverses of each other. Because additive inverses are unique this should
    not generally be possible. In fact, $U + W = \{\vec{0}\}$ if and only if
    $U = W = \{\vec{0}\}$.
    \begin{proof}
        The backwards direction is clear. \\
        ($\implies$) Assume $U + W = \{\vec{0}\}$. As with all subspaces
        $\{\vec{0}\} \subseteq U$ and $\{\vec{0}\} \subseteq W$. By
        \textsc{theorem 1.40} we have $U \subseteq \{\vec{0}\}$ and
        $W \subseteq \{\vec{0}\}$ and thus $U = W = \{\vec{0}\}$.
    \end{proof}
\end{exercise}

\begin{exercise}[19]
    This is false. For a counterexample let $V=\R^4$, $U=\text{span}
    (\vec{e_1},\vec{e_2})$, $V_1=\text{span}(\vec{e_1},\vec{e_3})$,
    and $V_2=\text{span}(\vec{e_2},\vec{e_3})$.
    Then $V_1 + U = V_2 + U$ however $V_1 \neq V_2$.
\end{exercise}

\begin{exercise}[20]
    Let $W =\text{span}(\vec{e_2},\vec{e_4})$.
    Then it's easy to see that $\F^4 = U \oplus W$.
\end{exercise}

\begin{exercise}[23]
    This is false. For a counterexample let $V=\R^2$, $U=\text{span}\{(0,1)\}$,
    $V_1=\text{span}\{(1,0)\}$ and $V_2=\text{span}\{(1,1)\}$.
    Then $V = V_1 \oplus U$ and $V = V_2 \oplus U$ however $V_1 \neq V_2$.
\end{exercise}

\pagebreak

\section{\underline{\Large{Exercises 2A}}} \vspace{5mm}

\begin{exercise}[1]
    The list $(1,0,-1),(0,1,-1),(3,0,-3),(0,2,-2)$ spans this subspace
    of $\F^3$. \\ We can see this in the equation,
    \begin{equation*}
        a(1,0,-1) + b(0,1,-1) + c(3,0,-3) + d(0,2,-2) = (a+3c,b+2d,-a-b-3c-2d)
    \end{equation*}
    where $a,b,c,d \in \F$.
\end{exercise}

\begin{exercise}[4]
    Part (a)
    \begin{proof}
        ($\implies$):
        Assume the list $\{\vec{v}\}$ is linearly independent. \\
        If $\vec{v}$ is the zero vector then we have $(1)\vec{v} = \vec{0}$,
        which contradicts the linear independence of $\{\vec{v}\}$.
        Thus we must have $\vec{v} \neq \vec{0}$. \\

        ($\impliedby$):
        Assume $\vec{v} \neq \vec{0}$. \\
        Then by \textsc{Exercise 2 of 1B}
        whenever $\alpha \vec{v} = \vec{0}$ where $\alpha \in \F$
        we must have $\alpha = 0$, which show that $\{\vec{v}\}$
        is linearly independent.
    \end{proof}
    Part (b)
    \begin{proof}
        ($\implies$):
        Assume the list $\{\vec{u},\vec{v}\}$ is linearly independent. \\
        If $\alpha \vec{u} = \vec{v}$ for some $\alpha \in \F$ (that is
        the vectors are scalar multiples of each other) then we can rearrange
        to get $\alpha \vec{u}-\vec{v}=0$, which contradicts the assumption
        that $\{\vec{u},\vec{v}\}$ is linearly independent. Thus, the two
        vectors in the list are not scalar multiples of each other. \\

        ($\impliedby$):
        Assume $\vec{u}$ and $\vec{v}$ are not scalar multiples of each other. \\
        This means there is no $\alpha \in \F$ such that $\alpha \vec{u} = \vec{v}$.
        If $\beta \vec{u} + \gamma \vec{v} = \vec{0}$ for $\beta, \gamma \in \F$ we
        must have $\beta = 0 = \gamma$, otherwise $-\frac{\beta}{\gamma}\vec{u} =
        \vec{v}$, which is a contradiction. Hence, the list $\{\vec{u},\vec{v}\}$
        is linearly independent.
    \end{proof}
\end{exercise}

\begin{exercise}[7]
    Part (a) \\
    Assuming $a,b \in \R$ are scalars we have
    \begin{align*}
    0 = a(1+i)+b(1-i) = a+b+i(a-b)
    \end{align*}
    which implies that
    \begin{align*}
    a+b=i(b-a).
    \end{align*}
    A real number cannot be a multiple of $i \in \C$ and
    so we must have $a+b=0$ and $b-a=0$, which has one
    solution $a=b=0$. Thus, the list $\{1+i,1-i\}$ is
    linearly independent. \\\\
    Part (b) \\
    We compute $i(1-i)=1+i$. So by \textsc{Exercise 4b of 2A}
    the list $\{1+i,1-i\}$ is linearly dependent.
\end{exercise}

\begin{exercise}[10]
    \begin{proof}
    Suppose $\vec{v_1},\ldots,\vec{v_m}$ is a linearly independent
    list of vectors and $\lambda \in \F$ with $\lambda \neq 0$. \\\\
    Then if $\alpha_1,\ldots,\alpha_m \in \F$ are scalars we have
    \begin{align*}
        \sum_{i=1}^{m} \alpha_i(\lambda \vec{v_i}) = \vec{0}
        &\implies \lambda \sum_{i=1}^{m} \alpha_i \vec{v_i} = \vec{0} \\\
        &\implies \sum_{i=1}^{m} \alpha_i \vec{v_i} = \vec{0} \ \ \text{(since $\lambda \neq 0$)} \\\
        &\implies \alpha_i = 0 \ \text{for all} \ 1 \leqslant i \leqslant m \ \ \text{(by linear independence of $\vec{v_1},\ldots,\vec{v_m}$)} \\\
    \end{align*}
    Thus, $\lambda \vec{v_1},\lambda \vec{v_2},\ldots,\lambda \vec{v_m}$ is a linearly independence list of vectors. \\
    \end{proof}
    Note also that $\text{span}(\lambda \vec{v_1},\ldots,\lambda \vec{v_m}) = \text{span}(\vec{v_1},\ldots,\vec{v_m})$
\end{exercise}

\begin{exercise}[11]
    This is false.
    For a counterexample, let $V = \R^2$ and take the two linearly
    independent lists $\{(1,0),(0,1)\}$ and $\{(0,-1),(-1,0)\}$.
    Then the new list is $\{(1,-1),(-1,1)\}$ which is linearly dependent.
\end{exercise}

\begin{exercise}[12]
    Note that this result is uninteresting if $\vec{w} = 0$. So further assume $\vec{w} \neq 0$.
    \begin{proof}
        Assume $\vec{v_1}+\vec{w},\ldots,\vec{v_m}+\vec{w}$ is linearly dependent. \\
        Then there exists $\alpha_1,\ldots,\alpha_m \in \F$, not all zero, such that
        $\alpha_1(\vec{v_1}+\vec{w})+\ldots+\alpha_m(\vec{v_m}+\vec{w})=\vec{0}$ and
        by rearranging terms we obtain $\alpha_1\vec{v_1}+\ldots+\alpha_m\vec{v_m}=
        -(\alpha_1+\ldots+\alpha_m)\vec{w}$. Because the vectors $\vec{v_1},\ldots,
        \vec{v_m}$ are linearly independent and the $\alpha_i$ are not all zero we
        must have $\alpha_1+\ldots+\alpha_m \neq 0$. Dividing through by $\beta =
        -(\alpha_1 + \ldots + \alpha_m)$ gives $\vec{w} = \frac{\alpha_1}{\beta}
        \vec{v_1} + \ldots + \frac{\alpha_m}{\beta}\vec{v_m}$ and thus $\vec{w}
        \in \text{span}(\vec{v_1},\ldots,\vec{v_m})$.
    \end{proof}
\end{exercise}

\begin{exercise}[13]
    Let $\vec{v_1},\ldots,\vec{v_m}$ be linearly independent in $V$ and $\vec{w} \in V$.
    \begin{proof}
        ($\implies$):
        Assume $\vec{v_1},\ldots,\vec{v_m},\vec{w}$ is linearly independent.
        By this assumption $\vec{w} \in V$ cannot be written as a linear
        combination of the vectors $\vec{v_1},\ldots,\vec{v_m} \in V$.
        Thus $\vec{w} \notin \text{span}(\vec{v_1},\ldots,\vec{v_m})$. \\

        ($\impliedby$):
        Assume $\vec{w} \notin \text{span}(\vec{v_1},\ldots,\vec{v_m})$.
        For the sake of a contradiction, further assume that the list
        $\vec{v_1},\ldots,\vec{v_m},\vec{w}$ is linearly dependent.
        Then by \textsc{theorem 2.19} at least one of these vectors
        is in the span of the previous ones. If $\vec{w} \in
        \text{span}(\vec{v_1},\ldots,\vec{v_m})$ then we clearly
        have a contradiction. But if one of the $v_i$'s is contained
        in the span of the vectors before it then this contradicts
        the linear independence of $\vec{v_1},\ldots,\vec{v_m}$.
        Thus the list $\vec{v_1},\ldots,\vec{v_m},\vec{w}$ must be
        linearly independent.
    \end{proof}
\end{exercise}

\begin{exercise}[15]
    We have $\mathcal{P}_4(\F)=\text{span}(1,z,z^2,z^3,z^4)$ which
    is a spanning set of length five. So by \textsc{theorem 2.22}
    there does not exist a list of six (or more) polynomials in
    $\mathcal{P}_4(\F)$ that is linearly independent.
\end{exercise}

\begin{exercise}[16]
    The list $1,z,z^2,z^3,z^4$ of five polynomials in $\mathcal{P}_4(\F)$
    is linearly independent. So by \textsc{theorem 2.22} there is no list of
    four polynomials that spans $\mathcal{P}_4(\F)$, otherwise we would
    have a contradiction.
\end{exercise}

\begin{exercise}[17]
    \begin{proof}
        ($\implies$):
        Assume that $V$ is infinite-dimensional. \\
        This means that there is no (finite) list
        of vectors that spans the vector space $V$. \\
        We construct the desired infinite sequence of vectors
        in $V$ through the following process. \\\\
        $\underline{\textbf{Step 1:}}$ \\
        Pick any vector $\vec{v_1} \in V$ and form the list
        $B = \{\vec{v_1}\}$, which is linearly independent. \\\\
        $\underline{\textbf{Step \textit{m}:}}$ \\
        Because $V$ is infinite-dimensional we have $\text{span}(B)
        \neq V$, and in particular $\text{span}(B) \subset V$.
        Choose a vector $v_m \in V$ that is not contained in
        $\text{span}(B)$ and append it to the end of $B$.
        Now repeat \textit{step m}. \\\\
        By \textsc{Exercise 13 of 2A} the resulting list $B$
        (of length $m \in \N$) in \textit{step m} is linearly independent.
        The process will never terminate since $V$ is infinite-dimensional.
        Thus these steps produce the desired infinite sequence of vectors. \\

        ($\impliedby$):
        Assume we have such an infinite sequence of vectors in $V$.
        Assume $V$ is finite-dimensional and that $V=\text{span}
        (\vec{v_1},\ldots,\vec{v_k})$, where this list of vectors
        is of length $k \in \N$. Using our infinite sequence we can
        obtain a linearly independent list of vectors of length
        $n \in \N$ where $n > k$. However this contradicts
        \textsc{theorem 2.22}. Hence, $V$ is infinite-dimensional.
    \end{proof}
\end{exercise}

\begin{exercise}[18]
    Consider the sequence of vectors $(1,0,0,0,\ldots),\ (0,1,
    0,0,\ldots),\ (0,0,1,0,\ldots),\ldots$ in $\F^{\infty}$ and
    apply the theorem from the previous exercise.
\end{exercise}

\begin{exercise}[19]
    TODO: I don't know currently how to prove this rigorously, but it
    seems related to the \textit{Weierstrass Approximation Theorem}.
\end{exercise}

\pagebreak

\section{\underline{\Large{Exercises 2B}}} \vspace{5mm}

\begin{exercise}[1]
    Assume that the vector space field is either $\R$ or $\C$, ignoring
    vector spaces over finite fields such as $\F_2$. \\\\
    The trivial vector space has exactly one basis, the empty list $\{\}$. \\
    Now assume $\vec{v_1},\vec{v_2},\ldots,\vec{v_n}$ is the only basis for
    a vector space $V$. The list $\vec{v_1}+\vec{v_2},\vec{v_2},\ldots,
    \vec{v_n}$ is also a basis for $V$ (this is easy to check). By the
    uniqueness of our original basis we must have $\vec{v_1}=\vec{v_1}+
    \vec{v_2}$ which implies that $\vec{v_2}=\vec{0}$, a contradiction
    since a basis cannot contain the zero vector. \\
    If $\text{dim}\ V = 1$ a similar argument shows that there cannot be
    a single unique basis of $V$. \\\\
    Hence the only vector space with exactly one basis is the trivial
    vector space $\{\vec{0}\}$ of dimension zero.
\end{exercise}

\begin{exercise}[3]
    (a) \\
    The list $(3,1,0,0,0),(0,0,7,1,0),\vec{e_5}$ is a basis of $U\subseteq\R^5$. \\\\
    (b) \\
    First append the standard basis of $\R^5$, $\vec{e_1},\vec{e_2},\vec{e_3},
    \vec{e_4},\vec{e_5}$, to the end of the list from part (a). Then apply the
    procedure from the proof of \textsc{theorem 2.30}, which will give this
    basis of $\R^5$: $(3,1,0,0,0),(0,0,7,1,0),\vec{e_5},\vec{e_1},\vec{e_3}$. \\\\
    (c) \\
    Here we follow the logic of the proof of \textsc{theorem 2.33} by defining
    the subspace $W=\text{span}(\vec{e_1},\vec{e_3})\subseteq\R^5$. \\
    It is clear that $U \cap W = \{\vec{0}\}$. Moreover if $(x_1,x_2,x_3,x_4,x_5)
    \in \R^5$ we can write:
    \begin{equation*}
    (x_1,x_2,x_3,x_4,x_5) = \underbrace{(3x_2,x_2,7x_4,x_4,x_5)}_\textsl{$\in$ U}
                        + \underbrace{(x_1-3x_2,0,x_3-7x_4,0,0)}_\textsl{$\in$ W}
    \end{equation*}
    which demonstrates that $\R^5 = U + W$. Thus $\R^5 = U \oplus W$.
\end{exercise}

\begin{exercise}[5]
    \begin{proof}
        By \textsc{theorems 2.25 and 2.31} the subspaces $U$ and $W$ have
        bases, say, $\vec{u_1},\ldots,\vec{u_m}$ and $\vec{w_1},\ldots,\vec{w_n}$,
        respectively. If $\vec{v} \in V$ we can write $\vec{v}=\vec{u}+\vec{w}$
        for some $\vec{u} \in U$ and $\vec{w} \in W$ (since $V=U+W$). We can also
        write $\vec{u}=\alpha_1\vec{u_1}+\cdots+\alpha_m\vec{u_m}$ and $\vec{w}=
        \beta_1\vec{w_1}+\cdots+\beta_n\vec{w_n}$ for some $\alpha_i,\beta_i \in \F$
        (since $\vec{u_1},\ldots,\vec{u_m}$ and $\vec{w_1},\ldots,\vec{w_n}$ are
        bases for $U$ and $W$). And so $\vec{v}$ can be expressed as $\vec{v}=
        \alpha_1\vec{u_1}+\cdots+\alpha_m\vec{u_m}+\beta_1\vec{w_1}+\cdots+
        \beta_n\vec{w_n}$ and thus $V=\text{span}(\vec{u_1},\ldots,\vec{u_m},
        \vec{w_1},\ldots,\vec{w_n})$. \\ Now apply the procedure from the
        proof of \textsc{theorem 2.30} to the list $\vec{u_1},\ldots,\vec{u_m},
        \vec{w_1},\ldots,\vec{w_n}$ and through this we obtain a basis for $V$.
        Since the procedure only removes vectors from the original list, the
        vectors in this basis are contained in $U \cup W$, completing the proof.
    \end{proof}
\end{exercise}

\pagebreak

\begin{exercise}[6]
    Here is a counterexample. \\
    Consider the list $1,x,x^3+x^2,-x^3+x^2$ in $\mathcal{P}_3(\F)$.
    None of these polynomials have degree two, however (using \textsc{theorem 2.38})
    they form a basis of $\mathcal{P}_3(\F)$ because they are linearly independent
    and the list is of length four.
\end{exercise}

\begin{exercise}[7]
    Assume that $\vec{w} \in V$.
    Then $\vec{w} = a_1 \vec{v_1} + a_2 \vec{v_2} + a_3 \vec{v_3} + a_4 \vec{v_4}$
    for some scalars $a_i \in \F$. \\
    First we compute,
    \begin{equation*}
        \begin{split}
        \vec{w} & = a_1 \vec{v_1} + a_2 \vec{v_2} + a_3 \vec{v_3} + a_4 \vec{v_4} \\
                & = a_1\vec{v_1} + a_1\vec{v_2} + (a_2-a_1)\vec{v_2} + (a_2-a_1)\vec{v_3} + (a_3-a_2+a_1)\vec{v_3} + (a_3-a_2+a_1)\vec{v_4} + (a_4-a_3+a_2-a_1)\vec{v_4} \\
                & = a_1(\vec{v_1}+\vec{v_2}) + (a_2-a_1)(\vec{v_2}+\vec{v_3}) + (a_3-a_2+a_1)(\vec{v_3}+\vec{v_4}) + (a_4-a_3+a_2-a_1)\vec{v_4}
        \end{split}
    \end{equation*}
    And thus $\vec{w} \in \text{span}(v_1+v_2,v_2+v_3,v_3+v_4,v_4)$ and this list
    of four vectors spans $V$. \\\\
    Secondly, $\vec{v_1}+\vec{v_2}\neq\vec{0}$ because $\vec{v_1}\neq-\vec{v_2}$.
    We have $\vec{v_2}+\vec{v_3}\notin\text{span}(\vec{v_1}+\vec{v_2})$ since this
    is logically equivalent to $\vec{v_3}\notin\text{span}(\vec{v_1},\vec{v_2})$.
    A similar argument shows $\vec{v_3}+\vec{v_4}\notin\text{span}(\vec{v_1}+\vec
    {v_2},\vec{v_2}+\vec{v_3})$ and the same holds for the last vector $\vec{v_4}$. \\
    So by \textsc{theorem 2.19} (the \textit{linear dependence lemma}) the list
    $v_1+v_2,v_2+v_3,v_3+v_4,v_4$ is linearly independent, and thus is a basis of $V$.
\end{exercise}

\begin{exercise}[8]
    Here is a counterexample. \\
    Consider the subspace $U = \text{span}(1,x,x^2)$ of $\mathcal{P}_3(\F)$ and the
    basis from \textsc{exercise 6}: $1,x,x^3+x^2,-x^3+x^2$. The first two basis
    vectors are contained in $U$ and the last two basis vectors are not contained
    in $U$. However, the first two basis vectors $1,x \in \mathcal{P}_3(\F)$ do not
    form a basis of $U$ (since $U$ is of dimension three).
\end{exercise}

\begin{exercise}[10]
    \begin{proof}
        For every $\vec{v} \in V$ we have $\vec{v}=\vec{u}+\vec{w}$
        for some $\vec{u} \in U$ and $\vec{w} \in W$.
        We can write $\vec{u}=\alpha_1\vec{u_1}+\cdots+\alpha_m\vec{u_m}$ and
        $\vec{w}=\beta_1\vec{w_1}+\cdots+\beta_n\vec{w_n}$ where $\alpha_i,\beta_i \in \F$.
        So we can express $\vec{v}$ as $\vec{v}=\alpha_1\vec{u_1}+\cdots+\alpha_m
        \vec{u_m}+\beta_1\vec{w_1}+\cdots+\beta_n\vec{w_n}$ and thus $V=\text{span}(
        \vec{u_1},\ldots,\vec{u_m},\vec{w_1},\ldots,\vec{w_n})$. \\\\
        To show linearly independence suppose $\alpha_1\vec{u_1}+\cdots+
        \alpha_m\vec{u_m}+\beta_1\vec{w_1}+\cdots+\beta_n\vec{w_n}=\vec{0}$.
        We can rearrange to get $U \ni \alpha_1\vec{u_1}+\cdots+\alpha_m\vec{u_m}
        =-\beta_1\vec{w_1}-\cdots-\beta_n\vec{w_n} \in W$ and because $U \cap W = \{\vec{0}\}$
        this implies that both vectors are zero and then using the fact that $\vec{u_1},\ldots,
        \vec{u_m}$ and $\vec{w_1},\ldots,\vec{w_n}$ are bases we conclude that $\alpha_1=\cdots
        =\alpha_m=\beta_1=\cdots=\beta_n=0$. Thus the list $\vec{u_1},\ldots,\vec{u_m},\vec{w_1}
        ,\ldots,\vec{w_n}$ is linearly independent and is a basis of $V$.
    \end{proof}
\end{exercise}

\pagebreak

\begin{exercise}[11]
    First observe that we can identify each basis vector $\vec{v_i} \in
    V$ with the "complexified" vector $\vec{v_i}+i\vec{0} \in V_\C$. \\\\
    We now prove that $\vec{v_1},\ldots,\vec{v_n}$ (which is shorthand for
    $\vec{v_1}+i\vec{0},\ldots,\vec{v_n}+i\vec{0}$) is a basis of $V_\C$.
    \begin{proof}
    Suppose $\vec{u}+i\vec{w} \in V_\C$. \\
    Because $\vec{v_1},\ldots,\vec{v_n}$ is a basis of $V$ we have $\vec{u}=
    \alpha_1\vec{v_1}+\ldots+\alpha_n\vec{v_n}$ for unique scalars $\alpha_k
    \in \R$ and $\vec{w}=\beta_1\vec{v_1}+\ldots+\beta_n\vec{v_n}$ for unique
    scalars $\beta_k \in \R$. We then can write,
    \begin{equation*}
        \vec{u}+i\vec{w}=(\alpha_1\vec{v_1}+\ldots+\alpha_n\vec{v_n})+
        i(\beta_1\vec{v_1}+\ldots+\beta_n\vec{v_n})=(\alpha_1+i\beta_1)
        \vec{v_1}+\ldots+(\alpha_n+i\beta_n)\vec{v_n}.
    \end{equation*}
    The complex numbers $\alpha_k+i\beta_k \in \C$ are unique because their
    real components are unique. Thus by \textsc{theorem 2.28} the list
    $\vec{v_1},\ldots,\vec{v_n}$ is a basis of the complex vector
    space $V_\C$.
    \end{proof}
    Note that the complexification of a real vector space can also be defined
    as a tensor product of two real vector spaces: $V_\C = V \otimes \C$.
\end{exercise}

\pagebreak

\section{\underline{\Large{Exercises 2C}}} \vspace{5mm}

\begin{exercise}[1]
    \begin{proof}
        By \textsc{theorem 2.37} all subspaces of $\R^2$ are of dimension
        zero, one or two. By \textsc{theorem 2.39} the only subspace of
        dimension two is $\R^2$ itself and clearly the only subspace of
        dimension zero is the trivial subspace $\{\vec{0}\}$. Hence it
        suffices to show that every subspace of dimension one is a line
        containing the origin. \\
        Suppose $U\subseteq\R^2$ is a subspace with $\text{dim}\ U = 1$
        and $\{\vec{u}\}$ is a basis of $U$ (which must exist by
        \textsc{theorem 2.31}). Then by the definition of a basis we
        have $U=\text{span}(\vec{u})=\{\lambda\vec{u}\mid\lambda\in\R\}$,
        which in $\R^2$ is a line containing the origin.
    \end{proof}
\end{exercise}

\begin{exercise}[2]
    \begin{proof}
        Similarly to the previous exercise, it suffices to show that every
        subspace of dimension one is a line containing the origin and every
        subspace of dimension two is a plane containing the origin. \\
        The same argument from the previous exercise shows that every
        one-dimensional subspace $U \subseteq \R^3$ is equal to
        $\text{span}(\vec{u})$ for some $\vec{u} \in \R^3$ (i.e. a
        straight line through the origin). \\
        Now suppose $U \subseteq \R^3$ is a two-dimensional subspace and
        $\{\vec{u},\vec{w}\}$ is a basis of $U$. Then $U=\text{span}(\vec
        {u},\vec{w})=\{\alpha\vec{u}+\beta\vec{w}\mid\alpha,\beta\in\R\}$
        (which is a plane containing the origin), completing the proof.
    \end{proof}
\end{exercise}

\begin{exercise}[11]
    Assume $\F=\R$. By \textsc{theorem 2.43} we have $\text{dim}(U+W)=
    \text{dim }U+\text{dim }W-\text{dim}(U \cap W)=8-\text{dim}(U \cap W)
    \leq\text{dim }\C^6=6$ where the inequality holds by \textsc{theorem
    2.37}. So $\text{dim}(U \cap W)\geq2$. Thus by the linear independence
    of any basis of $U \cap W$ there exists two vectors such that neither
    of these vectors is a scalar multiple of the other.
\end{exercise}

\begin{exercise}[12]
    We have $8=\text{dim }\R^8=\text{dim}(U+W)=\text{dim }U+\text{dim }W-
    \text{dim}(U \cap W)=3+5-\text{dim}(U \cap W)$ and so $\text{dim}(U\cap W)
    =0$ which implies that $U\cap W = \{\vec{0}\}$. Now applying
    \textsc{theorem 1.46} gives $\R^8 = U \oplus W$.
\end{exercise}

\begin{exercise}[13]
    We have $\text{dim}(U+W)=\text{dim }U+\text{dim }W-\text{dim}(U \cap W)=
    10-\text{dim}(U \cap W)\leq\text{dim }\R^9=9$. So $\text{dim}(U \cap W)
    \geq1$ and thus $U \cap W \neq \{\vec{0}\}$.
\end{exercise}

\pagebreak

\begin{exercise}[14]
    Using \textsc{theorem 2.43} and \textsc{theorem 2.37} we have
    \begin{equation*}
        \text{dim}(V_1+V_2)=\text{dim }V_1+\text{dim }V_2-
        \text{dim}(V_1 \cap V_2)=7+7-\text{dim}(V_1 \cap V_2)
        \leq 10=\text{dim }V
    \end{equation*}
    and so $\text{dim}(V_1 \cap V_2)\geq 4$. \\\\
    Also observe that by \textsc{theorem 2.37} we have $\text{dim}((V_1 \cap V_2)+V_3) \leq 10$. \\\\
    Then,
    \begin{equation*}
        \begin{split}
        \text{dim}(V_1 \cap V_2 \cap V_3) & = \text{dim}(V_1\cap V_2)+\text{dim }V_3-\text{dim}((V_1\cap V_2)+V_3) \\
                                          & = \text{dim}(V_1\cap V_2)+7-\text{dim}((V_1\cap V_2)+V_3) \\
                                          & \geq 11-\text{dim}((V_1\cap V_2)+V_3) \\
                                          & \geq 11-10 \\
                                          & = 1
        \end{split}
    \end{equation*}
    and thus $V_1 \cap V_2 \cap V_3$ is a non-trivial subspace of $V$.
\end{exercise}

\begin{exercise}[15]
    Assume that $\text{dim }V_1+\text{dim }V_2+\text{dim }V_3 > 2\cdot\text{dim }V$. \\
    We have,
    \begin{equation*}
        \begin{split}
        \text{dim}(V_1 \cap V_2 \cap V_3) & = \text{dim}(V_1\cap V_2)+\text{dim }V_3-\text{dim}((V_1\cap V_2)+V_3) \\
                                          & = \text{dim }V_1+\text{dim }V_2-\text{dim}(V_1+V_2)+\text{dim }V_3-\text{dim}((V_1\cap V_2)+V_3) \\
                                          & > 2\cdot\text{dim }V-\text{dim}(V_1+V_2)-\text{dim}((V_1\cap V_2)+V_3) \\
                                          & = [\text{dim }V-\text{dim}(V_1+V_2)]+[\text{dim }V-\text{dim}((V_1\cap V_2)+V_3)] \\
                                          & \geq 0
        \end{split}
    \end{equation*}
    and thus $V_1 \cap V_2 \cap V_3$ is a non-trivial subspace of $V$.
\end{exercise}

\begin{exercise}[17]
    \begin{proof}
    Suppose that $V_1,\ldots,V_m$ are finite-dimensional subspaces of $V$. \\
    We can prove this result straightforwardly using induction. Alternatively,
    by repeatedly applying \textsc{theorem 2.43} and the associativity of
    sums of subspaces we obtain the formula:

    \begin{equation*}
        \text{dim}(V_1+\cdots+V_m) = \sum_{j=1}^{m} \text{dim}\ V_j
        - \sum_{j=2}^{m} \text{dim}(V_{j-1} \cap (V_j+\cdots+V_m))
        \leq \sum_{j=1}^{m} \text{dim}\ V_j
    \end{equation*}

    And because the subspaces $V_j$ are finite-dimensional, the sum
    $V_1+\cdots+V_m$ is therefore finite-dimensional.
    Completing the proof.
    \end{proof}
\end{exercise}

\begin{exercise}[18]
    \begin{proof}
    Suppose $V$ is finite-dimensional, with $\text{dim}\ V = n \geq 1$. \\
    Let $\vec{v_1},\ldots,\vec{v_n}$ be a basis of $V$. For each $1 \leq
    j \leq n$ define the subspace $V_j = \text{span}(\vec{v_j})$. \\
    Because $\vec{v_1},\ldots,\vec{v_n}$ is linearly independent, the only
    way to write $\vec{0} \in V$ in the form $\vec{w_1}+\ldots+\vec{w_n}$ where
    each $\vec{w_k} \in V_k$ is by taking each $\vec{w_k}$ equal to $\vec{0}$,
    hence by \textsc{theorem 1.45} the sum $V_1+\cdots+V_m$ is a direct sum. \\
    Now assume $\vec{x} \in V$. Because $\vec{v_1},\ldots,\vec{v_n}$ spans $V$,
    we can write $\vec{x}=\alpha_1\vec{v_1}+\ldots+\alpha_n\vec{v_n}$ for some
    scalars $\alpha_i \in \F$. But each $\alpha_i\vec{v_i}$ is contained in the
    subspace $V_i$ and hence $\vec{x} \in V_1 \oplus \cdots \oplus V_n$. \\
    Thus $V = V_1 \oplus \cdots \oplus V_n$.
    \end{proof}
\end{exercise}

\end{document}
